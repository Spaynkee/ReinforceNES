{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Helpful information\n",
    "# https://stable-baselines.readthedocs.io/en/master/guide/examples.html#using-callback-monitoring-training\n",
    "# actual rom files must be named rom.nes and placed in their specific folders... my path is:\n",
    "# {path on colossus}\n",
    "# Scenario files also go here, these are what determines how we reward the agent\n",
    "\n",
    "# To Do:\n",
    "# See which of the models need special policies.\n",
    "# The plan was to try various baseline modesl and compare them, (Have each use 1m timesteps?)\n",
    "# try to auto-optimize their parameters and compare again, then\n",
    "# if we had time try to put together one of our own and see how it compares to theirs.\n",
    "# Haven't yet had good luck with any models thus far. (maybe because of the MlpPolicy?)\n",
    "\n",
    "# Game List: SMB, Life Force, Megaman? Must be NES games."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import retro\n",
    "import random\n",
    "import gym\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten\n",
    "from keras.layers.convolutional import Conv2D, Conv3D, Conv1D\n",
    "from keras.optimizers import Adam\n",
    "import os \n",
    "\n",
    "#mlp means Multilayer perceptron, and is probably the fastest but worst.\n",
    "#CnnPolicies are for images only. MlpPolicies are made for other type of features (e.g. robot joints) \n",
    "#Dunno what this means exactly, but I copied it from the documentation.\n",
    "from stable_baselines.common.policies import MlpPolicy, MlpLstmPolicy, MlpLnLstmPolicy, CnnPolicy, CnnLstmPolicy, CnnLnLstmPolicy\n",
    "from stable_baselines.deepq.policies import DQNPolicy\n",
    "from stable_baselines.common.vec_env import DummyVecEnv\n",
    "from stable_baselines import A2C, ACER, ACKTR, DDPG, DQN, GAIL, PPO2, SAC, TRPO\n",
    " #Documentation is here: https://stable-baselines.readthedocs.io/en/master/guide/examples.html#using-callback-monitoring-training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = retro.make(game=\"SuperMarioBros-Nes\")\n",
    "#env = retro.make(game=\"LifeForce-Nes\")\n",
    "#env = gym.make('CartPole-v0')\n",
    "env = DummyVecEnv([lambda: env])  # The algorithms require a vectorized environment to run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Probably want to add a timer here that concludes after training,\n",
    "# and also route output to some kind of file that we can read later.\n",
    "model = A2C(MlpPolicy, env, verbose=1)\n",
    "#model = PPO2(CnnPolicy, env, verbose=1)\n",
    "#model = ACER(MlpPolicy, env, verbose=1)\n",
    "#model = ACKTR(MlpPolicy, env, verbose=1)\n",
    "#model = DDPG(MlpPolicy, env, verbose=1)\n",
    "#model = DQN(DQNPolicy, env, verbose=1)\n",
    "#model = GAIL(MlpPolicy, env, verbose=1)\n",
    "#model = PPO2(MlpPolicy, env, verbose=1)\n",
    "#model = SAC(MlpPolicy, env, verbose=1)\n",
    "#model = TRPO(MlpPolicy, env, verbose=1)\n",
    "model.learn(total_timesteps=100000)\n",
    "#model.save(\"savedModel\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you want to just load a model instead of retraining... we can use this to train on colossus\n",
    "model = A2C.load(\"A2C5mMlp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = env.reset()\n",
    "for i in range(5000):\n",
    "    action, _states = model.predict(obs)\n",
    "    obs, rewards, dones, info = env.step(action)\n",
    "    env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To find out where to put .nes files\n",
    "print(retro.__file__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To see what games can be ran, so long as we aquire the rom files.\n",
    "retro.data.list_games()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
