{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Helpful information\n",
    "# https://stable-baselines.readthedocs.io/en/master/guide/examples.html#using-callback-monitoring-training\n",
    "# actual rom files must be named rom.nes and placed in their specific folders... my path is:\n",
    "# {path on colossus}\n",
    "# Scenario files also go here, these are what determines how we reward the agent\n",
    "\n",
    "# To Do:\n",
    "# Train a 5m timestep model for each of the 9 games.\n",
    "# Collect data, put it into a report. Prepare a presentation.\n",
    "# try to auto-optimize their parameters and compare again, then\n",
    "# Try to make our own, but time is pretty limited, so....\n",
    "\n",
    "# Hard: SMB, Life Force, Megaman\n",
    "# Medium: Breakout-Atari2600, Space Invaders, Asteroid\n",
    "# Easy: CartPole-v0, Pendulum-v0 MountainCar-v0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import retro\n",
    "import random\n",
    "import gym\n",
    "import numpy as np\n",
    "import os \n",
    "import time\n",
    "import datetime\n",
    "\n",
    "#mlp means Multilayer perceptron, and is probably the fastest but worst.\n",
    "#CnnPolicies are for images only. MlpPolicies are made for other type of features (e.g. robot joints) \n",
    "#Dunno what this means exactly, but I copied it from the documentation.\n",
    "from stable_baselines.common.policies import MlpPolicy, MlpLstmPolicy, MlpLnLstmPolicy, CnnPolicy, CnnLstmPolicy, CnnLnLstmPolicy\n",
    "from stable_baselines.deepq.policies import DQNPolicy\n",
    "from stable_baselines.common.vec_env import DummyVecEnv\n",
    "from stable_baselines import A2C, PPO2, TRPO\n",
    "# Documentation is here: https://stable-baselines.readthedocs.io/en/master/guide/examples.html#using-callback-monitoring-training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = {}\n",
    "output['highs'] = {}\n",
    "output['avgs'] = {}\n",
    "output['lows'] = {}\n",
    "\n",
    "#set timesteps up here now so we can get estimated time remaining, and finished elapsed time. Sorry.\n",
    "max_timesteps = 10000\n",
    "\n",
    "#Gotta use the right stats array or it'll blow.\n",
    "stats_ = ['fps', 'policy_entropy', 'value_loss'] #A2C\n",
    "#stats = ['fps', 'loss_val', 'explained_var', 'lr_now'] #PPO2\n",
    "#stats = ['t_start'] #TRPO Note: TRPO Doesn't have useful info in the callback, but it does if ran with verbose.\n",
    "# Still have to have 'stats' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_time(secs):\n",
    "    hours = 0\n",
    "    minutes = 0\n",
    "    \n",
    "    if secs > 3600:\n",
    "        hours = secs//3600\n",
    "        secs = secs - (hours*3600)\n",
    "    \n",
    "    if secs > 60:\n",
    "        minutes = secs//60\n",
    "        secs = secs - (minutes*60)\n",
    " \n",
    "    return int(hours), int(minutes), round(secs,2)\n",
    "\n",
    "def round_output():\n",
    "    for stat in stats:\n",
    "        output['highs'][stat] = round(output['highs'][stat], 4)\n",
    "        output['avgs'][stat] = round(output['avgs'][stat], 4)\n",
    "        output['lows'][stat] = round(output['lows'][stat], 4)\n",
    "        \n",
    "    return\n",
    "        \n",
    "def callback(_locals, _globals):\n",
    "    #print(_locals)\n",
    "    #return False\n",
    "    \n",
    "    # Apparently callback is called every 5th timestep for a2c, so we have to increment by 5. Nice.\n",
    "    \n",
    "\n",
    "    for stat in stats:\n",
    "        if stat not in output['highs'].keys():\n",
    "            output['highs'][stat] = _locals[stat]\n",
    "            \n",
    "        if stat not in output['lows'].keys():\n",
    "            output['lows'][stat] = _locals[stat]\n",
    "         \n",
    "        if stat not in output['avgs'].keys():\n",
    "            output['avgs'][stat] = _locals[stat]\n",
    "        else:\n",
    "            output['avgs'][stat] = (output['avgs'][stat] + _locals[stat])/2\n",
    "            \n",
    "    for stat in stats:\n",
    "        if output['highs'][stat] < _locals[stat]:\n",
    "             output['highs'][stat] = _locals[stat]\n",
    "                \n",
    "        if output['lows'][stat] > _locals[stat]:\n",
    "             output['lows'][stat] = _locals[stat]\n",
    "                \n",
    "    if 'nupdates' not in _locals.keys():\n",
    "        if 'timesteps_so_far' in _locals.keys():\n",
    "            total_timesteps = _locals['timesteps_so_far']\n",
    "            #Timesteps so far / elapsed time = ts/s, total - timesteps = remaining, remaining / t/s = how long we have.\n",
    "            curr_steps = _locals['timesteps_so_far']\n",
    "            elapsed_time = time.time() - start\n",
    "            ts_per_s = curr_steps / elapsed_time\n",
    "            remaining = (total_timesteps - curr_steps) \n",
    "        else:\n",
    "            curr_up = _locals['update'] * 5\n",
    "            total_timesteps = _locals['total_timesteps']\n",
    "            intervals = 600\n",
    "            if curr_up % intervals == 0:\n",
    "                elapsed_time = time.time() - start\n",
    "                ups_per_sec = curr_up / elapsed_time\n",
    "                remaining = (total_timesteps - curr_up)/ups_per_sec\n",
    "                hour_remain, minute_remain, second_remain = format_time(remaining)\n",
    "                print (f\"approx: training time remaining: {hour_remain}:{minute_remain}:{second_remain} {curr_up}/{total_timesteps}\")\n",
    "\n",
    "\n",
    "        return True\n",
    "    \n",
    "    curr_up = _locals['update']\n",
    "    nupdates = _locals['nupdates']\n",
    "    \n",
    "    elapsed_time = time.time() - start\n",
    "    ups_per_sec = curr_up / elapsed_time\n",
    "    remaining = (nupdates - curr_up)/ups_per_sec\n",
    "    hour_remain, minute_remain, second_remain = format_time(remaining)\n",
    "    print (f\"approx: training time remaining: {hour_remain}:{minute_remain}:{second_remain} {curr_up}/{nupdates}\")\n",
    "\n",
    "    return True\n",
    "\n",
    "def print_output():\n",
    "    round_output()\n",
    "    for stat in stats:\n",
    "        print(f\"{stat} low: {output['lows'][stat]} mean: {output['avgs'][stat]} high: {output['highs'][stat]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#env = retro.make(game=\"SuperMarioBros-Nes\")\n",
    "#env = retro.make(game=\"LifeForce-Nes\")\n",
    "#env = retro.make(game=\"MegaMan-Nes\")\n",
    "#env = retro.make(game=\"Asteroids-Atari2600\")\n",
    "env = retro.make(game=\"Breakout-Atari2600\") \n",
    "#env = retro.make(game=\"SpaceInvaders-Atari2600\") \n",
    "#env = gym.make('Pendulum-v0')\n",
    "#env = gym.make('CartPole-v0')\n",
    "#env = gym.make('MountainCar-v0')\n",
    "env = DummyVecEnv([lambda: env])  # The algorithms require a vectorized environment to run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "model = A2C(MlpPolicy, env, verbose=1)\n",
    "#model = PPO2(MlpPolicy, env, verbose=1)\n",
    "#model = TRPO(MlpPolicy, env, verbose=1)\n",
    "#model.learn(total_timesteps=max_timesteps, callback=callback)\n",
    "#model.save(\"savedModel\")\n",
    "end = time.time()\n",
    "\n",
    "hours, minutes, seconds = format_time(end-start)\n",
    "print(f\"Time Elapsed: {hours}:{minutes}:{seconds}\")\n",
    "print_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you want to just load a model instead of retraining\n",
    "#model = A2C.load(\"LFA2CMlp5m.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = env.reset()\n",
    "episode_reward = 0\n",
    "for i in range(5000):\n",
    "    action, _states = model.predict(obs)\n",
    "    obs, rewards, dones, info = env.step(action)\n",
    "    env.render()\n",
    "    episode_reward += rewards\n",
    "    if dones:\n",
    "        print('Reward: %s' % episode_reward)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To find out where to put rom files\n",
    "print(retro.__file__)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
